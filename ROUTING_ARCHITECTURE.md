# üåê Universal API Gateway - Complete Routing Architecture

## Overview

droid2api acts as a **universal API gateway** that accepts requests in three different API formats (OpenAI, Anthropic, Gemini) and routes them to various backends, returning responses in the original format requested.

---

## üéØ Multi-Format Support

### Supported Input Formats

```
droid2api Universal Gateway
         ‚îÇ
         ‚îú‚îÄ OpenAI Format:    POST /v1/chat/completions
         ‚îÇ                    Content-Type: application/json
         ‚îÇ                    { "model": "...", "messages": [...] }
         ‚îÇ
         ‚îú‚îÄ Anthropic Format: POST /v1/messages
         ‚îÇ                    Content-Type: application/json
         ‚îÇ                    anthropic-version: 2023-06-01
         ‚îÇ                    { "model": "...", "messages": [...] }
         ‚îÇ
         ‚îî‚îÄ Gemini Format:    POST /v1/generateContent
                              Content-Type: application/json
                              { "model": "...", "contents": [...] }
```

**Key Feature**: Request arrives in one format ‚Üí Transforms internally ‚Üí Routes to backend ‚Üí Returns in **original format**

---

## üîÑ Two Routing Modes

### Mode 1: Direct Backend Routing (Z.ai Example)

Route directly to a backend provider like Z.ai:

```bash
# Environment Configuration
export ANTHROPIC_MODEL=glm-4.6
export ANTHROPIC_BASE_URL=https://api.z.ai/api/anthropic
export ANTHROPIC_AUTH_TOKEN=665b963943b647dc9501dff942afb877.A47LrMc7sgGjyfBJ
```

**Flow:**
```
Client (any format)
        ‚Üì
droid2api (port 3000)
    ‚Ä¢ Receives request (OpenAI/Anthropic/Gemini)
    ‚Ä¢ Transforms to backend format (Anthropic for Z.ai)
    ‚Ä¢ Adds authentication
        ‚Üì
Z.ai Backend
    ‚Ä¢ GLM-4.6 model processes request
        ‚Üì
droid2api
    ‚Ä¢ Receives response
    ‚Ä¢ Transforms back to original format
        ‚Üì
Client receives response in original format
```

**Use Case**: Direct, simple routing to a single backend provider

---

### Mode 2: Smart Routing via claude-code-router

Route through claude-code-router for intelligent model selection:

```bash
# Environment Configuration
export OPENAI_API_KEY="sk-k2think-proxy-1760386095"
export OPENAI_BASE_URL="http://localhost:7000/v1"
export OPENAI_MODEL="MBZUAI-IFM/K2-Think"
```

**Flow:**
```
Client (any format)
        ‚Üì
claude-code-router (port 7000)
    ‚Ä¢ Token counting & analysis
    ‚Ä¢ Context size evaluation
    ‚Ä¢ Smart model selection based on:
        - thinking=true ‚Üí reasoning model
        - background=true ‚Üí cheap model
        - tokens > 60k ‚Üí long context model
        - default ‚Üí balanced model
        ‚Üì
    Routes to: droid2api (port 3000)
        ‚Üì
droid2api
    ‚Ä¢ Receives routed request (OpenAI format)
    ‚Ä¢ Transforms to backend format
    ‚Ä¢ Adds authentication
        ‚Üì
Backend Provider (Z.ai, Anthropic, OpenAI, etc.)
    ‚Ä¢ Model processes request
        ‚Üì
droid2api
    ‚Ä¢ Receives response
    ‚Ä¢ Transforms to OpenAI format
        ‚Üì
claude-code-router
    ‚Ä¢ Passes through response
        ‚Üì
Client receives response in OpenAI format
```

**Use Case**: Intelligent routing based on request characteristics for cost optimization

---

## üìä Complete Request Flow Examples

### Example 1: Gemini SDK ‚Üí Direct Z.ai

**Client Code:**
```javascript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI({
  apiKey: "dummy-key",
  baseUrl: "http://localhost:3000"  // droid2api
});

const model = genAI.getGenerativeModel({ model: "glm-4.6" });
const result = await model.generateContent("Explain quantum computing");
```

**Flow:**
```
1. Gemini SDK sends request
   POST http://localhost:3000/v1/generateContent
   {
     "model": "glm-4.6",
     "contents": [{
       "role": "user",
       "parts": [{"text": "Explain quantum computing"}]
     }]
   }

2. droid2api receives Gemini format
   ‚Ä¢ Detects /v1/generateContent endpoint
   ‚Ä¢ Calls geminiToOpenAI() transformer
   ‚Ä¢ Converts to OpenAI format internally:
     {
       "model": "glm-4.6",
       "messages": [{
         "role": "user",
         "content": "Explain quantum computing"
       }]
     }

3. droid2api transforms to backend
   ‚Ä¢ Model type: "anthropic" (from config)
   ‚Ä¢ Transforms OpenAI ‚Üí Anthropic format
   ‚Ä¢ Adds auth: ANTHROPIC_AUTH_TOKEN
   ‚Ä¢ Sends to: https://api.z.ai/api/anthropic

4. Z.ai processes with GLM-4.6
   ‚Ä¢ Returns Anthropic format response

5. droid2api transforms back
   ‚Ä¢ Anthropic ‚Üí OpenAI (internal)
   ‚Ä¢ OpenAI ‚Üí Gemini format
   ‚Ä¢ Returns to client:
     {
       "candidates": [{
         "content": {
           "parts": [{"text": "Quantum computing is..."}],
           "role": "model"
         },
         "finishReason": "STOP"
       }],
       "usageMetadata": {
         "promptTokenCount": 24,
         "candidatesTokenCount": 156,
         "totalTokenCount": 180
       }
     }

6. Gemini SDK receives native Gemini response
   ‚úÖ Success! Client sees perfect Gemini format
```

---

### Example 2: OpenAI SDK ‚Üí Smart Router ‚Üí K2-Think

**Client Code:**
```javascript
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "sk-k2think-proxy-1760386095",
  baseURL: "http://localhost:7000/v1"  // claude-code-router
});

const response = await openai.chat.completions.create({
  model: "MBZUAI-IFM/K2-Think",
  messages: [{
    role: "user",
    content: "Solve this complex reasoning problem..."
  }]
});
```

**Flow:**
```
1. OpenAI SDK sends request
   POST http://localhost:7000/v1/chat/completions
   {
     "model": "MBZUAI-IFM/K2-Think",
     "messages": [{
       "role": "user",
       "content": "Solve this complex reasoning problem..."
     }]
   }

2. claude-code-router analyzes
   ‚Ä¢ Token count: 5,432 tokens
   ‚Ä¢ Context: reasoning task detected
   ‚Ä¢ Decision: Use K2-Think reasoning model
   ‚Ä¢ Routes to: droid2api at localhost:3000

3. claude-code-router forwards
   POST http://localhost:3000/v1/chat/completions
   {
     "model": "MBZUAI-IFM/K2-Think",
     "messages": [...]
   }

4. droid2api processes
   ‚Ä¢ OpenAI format received
   ‚Ä¢ Model type: "openai" (or "common")
   ‚Ä¢ Adds authentication
   ‚Ä¢ Calls K2-Think API

5. K2-Think responds
   ‚Ä¢ Returns OpenAI format response
   ‚Ä¢ Includes reasoning tokens

6. droid2api passes through
   ‚Ä¢ OpenAI format maintained
   ‚Ä¢ Returns to claude-code-router

7. claude-code-router passes through
   ‚Ä¢ Logs usage statistics
   ‚Ä¢ Returns to client

8. OpenAI SDK receives response
   ‚úÖ Success! Native OpenAI format
```

---

### Example 3: Anthropic SDK ‚Üí Direct Z.ai

**Client Code:**
```javascript
import Anthropic from "@anthropic-ai/sdk";

const anthropic = new Anthropic({
  apiKey: "665b963943b647dc9501dff942afb877.A47LrMc7sgGjyfBJ",
  baseURL: "http://localhost:3000"  // droid2api
});

const message = await anthropic.messages.create({
  model: "glm-4.6",
  max_tokens: 1024,
  messages: [{
    role: "user",
    content: "Write a Python function to sort a list"
  }]
});
```

**Flow:**
```
1. Anthropic SDK sends request
   POST http://localhost:3000/v1/messages
   Headers:
     anthropic-version: 2023-06-01
   Body:
     {
       "model": "glm-4.6",
       "max_tokens": 1024,
       "messages": [{
         "role": "user",
         "content": "Write a Python function..."
       }]
     }

2. droid2api receives Anthropic format
   ‚Ä¢ Detects /v1/messages endpoint
   ‚Ä¢ Native Anthropic format
   ‚Ä¢ Adds auth from ANTHROPIC_AUTH_TOKEN
   ‚Ä¢ Routes directly to backend

3. Z.ai processes
   ‚Ä¢ GLM-4.6 model
   ‚Ä¢ Returns Anthropic format response

4. droid2api passes through
   ‚Ä¢ Anthropic format maintained
   ‚Ä¢ Returns to client

5. Anthropic SDK receives response
   ‚úÖ Success! Native Anthropic format
```

---

## üèóÔ∏è Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                 ‚îÇ
‚îÇ                    CLIENT APPLICATIONS                          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Google AI SDK ‚îÇ  ‚îÇ   OpenAI SDK   ‚îÇ  ‚îÇ Anthropic SDK  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Gemini)      ‚îÇ  ‚îÇ   (GPT)        ‚îÇ  ‚îÇ  (Claude)      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ           ‚îÇ                   ‚îÇ                    ‚îÇ           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                   ‚îÇ                    ‚îÇ
            ‚îÇ Gemini Format     ‚îÇ OpenAI Format      ‚îÇ Anthropic Format
            ‚îÇ                   ‚îÇ                    ‚îÇ
            ‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                            ‚îÇ
            ‚îÇ                            ‚ñº
            ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ              ‚îÇ  claude-code-router     ‚îÇ
            ‚îÇ              ‚îÇ  (Port 7000)            ‚îÇ
            ‚îÇ              ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
            ‚îÇ              ‚îÇ  ‚îÇ Smart Routing    ‚îÇ   ‚îÇ
            ‚îÇ              ‚îÇ  ‚îÇ ‚Ä¢ Token Analysis ‚îÇ   ‚îÇ
            ‚îÇ              ‚îÇ  ‚îÇ ‚Ä¢ Context Check  ‚îÇ   ‚îÇ
            ‚îÇ              ‚îÇ  ‚îÇ ‚Ä¢ Model Selection‚îÇ   ‚îÇ
            ‚îÇ              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
            ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                        ‚îÇ
            ‚îÇ                        ‚îÇ OpenAI Format
            ‚îÇ                        ‚îÇ (all requests)
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     ‚îÇ                ‚îÇ
                                     ‚ñº                ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ          droid2api (Port 3000)               ‚îÇ
              ‚îÇ        Universal API Gateway                  ‚îÇ
              ‚îÇ                                              ‚îÇ
              ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
              ‚îÇ  ‚îÇ  Format Detection & Transformation     ‚îÇ ‚îÇ
              ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
              ‚îÇ  ‚îÇ  ‚Ä¢ Gemini ‚Üí OpenAI ‚Üí Backend          ‚îÇ ‚îÇ
              ‚îÇ  ‚îÇ  ‚Ä¢ OpenAI ‚Üí Backend                   ‚îÇ ‚îÇ
              ‚îÇ  ‚îÇ  ‚Ä¢ Anthropic ‚Üí Backend                ‚îÇ ‚îÇ
              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
              ‚îÇ                                              ‚îÇ
              ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
              ‚îÇ  ‚îÇ  Authentication & Configuration        ‚îÇ ‚îÇ
              ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
              ‚îÇ  ‚îÇ  ‚Ä¢ ANTHROPIC_AUTH_TOKEN               ‚îÇ ‚îÇ
              ‚îÇ  ‚îÇ  ‚Ä¢ FACTORY_API_KEY                    ‚îÇ ‚îÇ
              ‚îÇ  ‚îÇ  ‚Ä¢ Model routing rules                ‚îÇ ‚îÇ
              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
              ‚îÇ                                              ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚îÇ Backend Format
                             ‚îÇ (Anthropic/OpenAI/Common)
                             ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ                  ‚îÇ                  ‚îÇ
          ‚ñº                  ‚ñº                  ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ   Z.ai       ‚îÇ  ‚îÇ  Anthropic   ‚îÇ  ‚îÇ   OpenAI     ‚îÇ
  ‚îÇ  (GLM-4.6)   ‚îÇ  ‚îÇ  (Claude)    ‚îÇ  ‚îÇ   (GPT)      ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                  ‚îÇ                  ‚îÇ
         ‚îÇ                  ‚îÇ                  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ Response
                            ‚îÇ
                            ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  droid2api               ‚îÇ
              ‚îÇ  Response Transformation ‚îÇ
              ‚îÇ  Backend ‚Üí Original      ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ               ‚îÇ               ‚îÇ
         ‚ñº               ‚ñº               ‚ñº
    Gemini Format   OpenAI Format   Anthropic Format
         ‚îÇ               ‚îÇ               ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
                    CLIENT
```

---

## üîß Configuration Examples

### Configuration 1: Direct Z.ai Routing

**Environment Variables:**
```bash
# droid2api config
export ANTHROPIC_MODEL=glm-4.6
export ANTHROPIC_BASE_URL=https://api.z.ai/api/anthropic
export ANTHROPIC_AUTH_TOKEN=665b963943b647dc9501dff942afb877.A47LrMc7sgGjyfBJ

# Start droid2api
npm start  # Port 3000
```

**droid2api config.json:**
```json
{
  "port": 3000,
  "models": [{
    "id": "glm-4.6",
    "name": "GLM-4.6",
    "type": "anthropic"
  }],
  "endpoints": [{
    "type": "anthropic",
    "base_url": "https://api.z.ai/api/anthropic"
  }]
}
```

**Client Usage:**
```bash
# Works with ANY SDK!

# OpenAI SDK
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"glm-4.6","messages":[{"role":"user","content":"Hello"}]}'

# Anthropic SDK
curl -X POST http://localhost:3000/v1/messages \
  -H "anthropic-version: 2023-06-01" \
  -d '{"model":"glm-4.6","max_tokens":100,"messages":[{"role":"user","content":"Hello"}]}'

# Gemini SDK
curl -X POST http://localhost:3000/v1/generateContent \
  -d '{"model":"glm-4.6","contents":[{"role":"user","parts":[{"text":"Hello"}]}]}'
```

---

### Configuration 2: Smart Routing via claude-code-router

**Environment Variables:**
```bash
# claude-code-router config
export OPENAI_API_KEY="sk-k2think-proxy-1760386095"
export OPENAI_BASE_URL="http://localhost:7000/v1"
export OPENAI_MODEL="MBZUAI-IFM/K2-Think"

# Start claude-code-router
ccr start  # Port 7000

# Start droid2api
npm start  # Port 3000
```

**claude-code-router config.json:**
```json
{
  "PORT": 7000,
  "Router": {
    "default": "droid2api,glm-4.6",
    "background": "droid2api,gpt-4o-mini",
    "think": "droid2api,K2-Think",
    "longContext": "droid2api,gemini-2.5-pro"
  },
  "Providers": [{
    "name": "droid2api",
    "api_base_url": "http://localhost:3000/v1/chat/completions",
    "api_key": "dummy",
    "models": ["glm-4.6", "K2-Think", "gemini-2.5-pro", "gpt-4o-mini"]
  }]
}
```

**droid2api config.json:**
```json
{
  "port": 3000,
  "models": [
    {
      "id": "glm-4.6",
      "type": "anthropic",
      "reasoning": "high"
    },
    {
      "id": "K2-Think",
      "type": "openai",
      "reasoning": "high"
    },
    {
      "id": "gemini-2.5-pro",
      "type": "openai"
    },
    {
      "id": "gpt-4o-mini",
      "type": "openai"
    }
  ],
  "endpoints": [
    {
      "type": "anthropic",
      "base_url": "https://api.z.ai/api/anthropic"
    },
    {
      "type": "openai",
      "base_url": "https://api.openai.com/v1"
    }
  ]
}
```

**Client Usage:**
```javascript
// Client only knows about router
const openai = new OpenAI({
  apiKey: "sk-k2think-proxy-1760386095",
  baseURL: "http://localhost:7000/v1"
});

// Router automatically selects best model
const response = await openai.chat.completions.create({
  model: "auto",  // Router decides!
  messages: [{ role: "user", content: "Complex reasoning task..." }]
});

// Router routes to K2-Think via droid2api
// droid2api handles format transformation
// Response comes back in OpenAI format
```

---

## üí∞ Cost Optimization with Smart Routing

### Routing Rules

```
Request Analysis:
‚îú‚îÄ Has thinking=true?        ‚Üí DeepSeek-reasoner ($0.55/1M)
‚îú‚îÄ Is background task?       ‚Üí GPT-4o-mini      ($0.60/1M)
‚îú‚îÄ Token count > 60k?        ‚Üí Gemini 2.5 Pro   (Free)
‚îî‚îÄ Default interactive?      ‚Üí GLM-4.6          ($2.00/1M via Z.ai)
```

### Cost Comparison

**Traditional (No Router):**
```
All requests ‚Üí Claude Opus 4 ‚Üí $15/1M tokens
Monthly cost: $1,350
```

**Optimized (With Router + droid2api):**
```
Interactive  ‚Üí GLM-4.6        ‚Üí $2.00/1M
Background   ‚Üí GPT-4o-mini    ‚Üí $0.60/1M
Reasoning    ‚Üí K2-Think       ‚Üí $0.55/1M
Long Context ‚Üí Gemini 2.5 Pro ‚Üí Free
Monthly cost: $276 (79.6% savings!)
```

---

## üéØ Use Cases

### Use Case 1: Multi-SDK Application

**Scenario**: Application uses different AI SDKs in different modules

```javascript
// Module 1: Uses Gemini SDK (for image analysis)
import { GoogleGenerativeAI } from "@google/generative-ai";
const gemini = new GoogleGenerativeAI({ baseUrl: "http://localhost:3000" });

// Module 2: Uses OpenAI SDK (for text generation)
import OpenAI from "openai";
const openai = new OpenAI({ baseURL: "http://localhost:3000/v1" });

// Module 3: Uses Anthropic SDK (for long-form content)
import Anthropic from "@anthropic-ai/sdk";
const anthropic = new Anthropic({ baseURL: "http://localhost:3000" });

// All three modules hit the same backend!
// All three get responses in their native format!
```

**Benefits:**
- ‚úÖ Use best SDK for each task
- ‚úÖ Single backend configuration
- ‚úÖ Easy to switch backends
- ‚úÖ Consistent authentication

---

### Use Case 2: Cost-Aware Development

**Scenario**: Optimize costs during development vs production

```bash
# Development: Use cheap local model
export ANTHROPIC_BASE_URL=http://localhost:11434/v1
export ANTHROPIC_MODEL=llama3.1

# Staging: Use mid-tier cloud model
export ANTHROPIC_BASE_URL=https://api.z.ai/api/anthropic
export ANTHROPIC_MODEL=glm-4.6

# Production: Use premium model
export ANTHROPIC_BASE_URL=https://api.anthropic.com
export ANTHROPIC_MODEL=claude-opus-4
```

**Benefits:**
- ‚úÖ No code changes needed
- ‚úÖ Environment-based routing
- ‚úÖ Cost control per environment
- ‚úÖ Easy testing with cheap models

---

### Use Case 3: Smart Model Selection

**Scenario**: Let router choose best model per request

```javascript
// Client doesn't specify model details
const response = await openai.chat.completions.create({
  model: "auto",
  messages: [{
    role: "user",
    content: "Analyze this 100-page document..."
  }]
});

// Router sees:
// - Token count: 80,000 tokens
// - Decision: Use Gemini 2.5 Pro (free, supports 2M context)
// - Routes to: droid2api ‚Üí Gemini 2.5 Pro
// - Cost: $0.00 instead of $240 with Claude!
```

**Benefits:**
- ‚úÖ Automatic cost optimization
- ‚úÖ Best model for each task
- ‚úÖ No client-side logic needed
- ‚úÖ Centralized routing rules

---

## üß™ Testing All Formats

```bash
# Test script: test-all-formats.sh

#!/bin/bash

BASE_URL="http://localhost:3000"

echo "Testing OpenAI format..."
curl -X POST $BASE_URL/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "glm-4.6",
    "messages": [{"role": "user", "content": "Say hello"}]
  }'

echo "\n\nTesting Anthropic format..."
curl -X POST $BASE_URL/v1/messages \
  -H "Content-Type: application/json" \
  -H "anthropic-version: 2023-06-01" \
  -d '{
    "model": "glm-4.6",
    "max_tokens": 100,
    "messages": [{"role": "user", "content": "Say hello"}]
  }'

echo "\n\nTesting Gemini format..."
curl -X POST $BASE_URL/v1/generateContent \
  -H "Content-Type: application/json" \
  -d '{
    "model": "glm-4.6",
    "contents": [{
      "role": "user",
      "parts": [{"text": "Say hello"}]
    }]
  }'

echo "\n\n‚úÖ All three formats should return 'Hello!' in their respective formats"
```

---

## üîç Troubleshooting

### Issue: "Model not found"

**Problem**: Client requests a model that doesn't exist in config

**Solution**: Add model to droid2api config.json:
```json
{
  "models": [{
    "id": "your-model-name",
    "type": "anthropic",  // or "openai"
    "reasoning": "high"   // optional
  }]
}
```

---

### Issue: "Authentication failed"

**Problem**: Backend rejects auth token

**Solution**: Check environment variables:
```bash
# For direct routing
echo $ANTHROPIC_AUTH_TOKEN

# For router
echo $OPENAI_API_KEY

# Restart services after changing env vars
npm restart
```

---

### Issue: "Format transformation error"

**Problem**: Response doesn't match expected format

**Solution**: Check endpoint path:
```
‚úÖ /v1/chat/completions    ‚Üí OpenAI format
‚úÖ /v1/messages            ‚Üí Anthropic format
‚úÖ /v1/generateContent     ‚Üí Gemini format
‚ùå /v1/completions         ‚Üí Wrong endpoint!
```

---

## üìö Related Documentation

- **Gemini Support**: See [GEMINI_SUPPORT.md](./GEMINI_SUPPORT.md) for detailed Gemini API documentation
- **Router Architecture**: See [claude-code-router INTEGRATION_DIAGRAM.md](https://github.com/musistudio/claude-code-router/blob/main/INTEGRATION_DIAGRAM.md)
- **Configuration Guide**: See [README.md](./README.md) for basic setup

---

## üéâ Summary

droid2api is a **universal API gateway** that:

‚úÖ **Accepts 3 formats**: OpenAI, Anthropic, Gemini
‚úÖ **Routes to any backend**: Z.ai, Anthropic, OpenAI, local models
‚úÖ **Returns original format**: Client gets response in format they sent
‚úÖ **Works with any SDK**: Use Google AI, OpenAI, or Anthropic SDKs
‚úÖ **Smart routing optional**: Add claude-code-router for intelligent model selection
‚úÖ **Cost optimization**: Route expensive tasks to cheap models automatically

**One gateway, infinite possibilities!** üöÄ

